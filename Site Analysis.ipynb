{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78acc19",
   "metadata": {},
   "source": [
    "# Subreddit Growth: How many unique subreddits were there at the beginning of 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a282b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from hdfs cluster on orion11\n",
    "# In here, I am using reservoir sample because the spark running out of memory and unable to read the full data set\n",
    "df_2018_jan = spark.read.json('hdfs://orion11:23001/RES-RC_2018-01.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b041c74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- can_gild: boolean (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_submitter: boolean (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- subreddit_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get schema\n",
    "df_2018_jan.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4618713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50974"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2018_jan.createOrReplaceTempView(\"df_view_2018_jan\")\n",
    "\n",
    "# Subreddit Growth: How many unique subreddits were there at the beginning of 2018?\n",
    "spark.sql(\"SELECT DISTINCT subreddit AS unique_subreddits FROM df_view_2018_jan\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121302da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018_dec = spark.read.json('hdfs://orion11:23001/RES-RC_2018-12.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f719a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- archived: boolean (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_created_utc: long (nullable = true)\n",
      " |-- author_flair_background_color: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_richtext: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- a: string (nullable = true)\n",
      " |    |    |-- e: string (nullable = true)\n",
      " |    |    |-- t: string (nullable = true)\n",
      " |    |    |-- u: string (nullable = true)\n",
      " |-- author_flair_template_id: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- author_flair_text_color: string (nullable = true)\n",
      " |-- author_flair_type: string (nullable = true)\n",
      " |-- author_fullname: string (nullable = true)\n",
      " |-- author_patreon_flair: boolean (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- can_gild: boolean (nullable = true)\n",
      " |-- can_mod_post: boolean (nullable = true)\n",
      " |-- collapsed: boolean (nullable = true)\n",
      " |-- collapsed_reason: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- gildings: struct (nullable = true)\n",
      " |    |-- gid_1: long (nullable = true)\n",
      " |    |-- gid_2: long (nullable = true)\n",
      " |    |-- gid_3: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_submitter: boolean (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- no_follow: boolean (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- removal_reason: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- send_replies: boolean (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- subreddit_name_prefixed: string (nullable = true)\n",
      " |-- subreddit_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2018_dec.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02bfe731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56146"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2018_dec.createOrReplaceTempView(\"df_view_2018_dec\")\n",
    "\n",
    "# Subreddit Growth: How many unique subreddits were there at the end of 2018?\n",
    "spark.sql(\"SELECT DISTINCT(subreddit) AS unique_subreddits FROM df_view_2018_dec\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58991611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique subreddits at the beginning of 2018: 50974\n",
      "The number of unique subreddits at the end of 2018: 56146\n"
     ]
    }
   ],
   "source": [
    "# Print output answer\n",
    "print(\"The number of unique subreddits at the beginning of 2018: {}\".format(Out[5]))\n",
    "print(\"The number of unique subreddits at the end of 2018: {}\".format(Out[8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f39e3b",
   "metadata": {},
   "source": [
    "Using the reservoir sample, we find out that there are about 50974 unique subreddit at the beginning of 2018 and 56146 unique subreddit at the end of 2018. This number might not be accurate as I am using reservoir sample of the data, but it should be close enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a363f53",
   "metadata": {},
   "source": [
    "# User Growth: How many active users does Reddit have now compared to the past?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05627a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from hdfs cluster on orion11\n",
    "# Reading the very first data reddit has vs the last data from reddit\n",
    "df_2005_jan = spark.read.json('hdfs://orion11:23001/RC_2005-12.bz2')\n",
    "df_2020_dec = spark.read.json('hdfs://orion11:23001/RES-RC_2020-12.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db04658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: boolean (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get 2005 schema\n",
    "df_2005_jan.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 2020 schema\n",
    "# df_2020_dec.printSchema()\n",
    "# The scheema is too long so I will just comment it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae7f007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# January 2005\n",
    "# So my assumption for active user is a user that post something (comment/subreddit) within the month that we are analyzing.\n",
    "df_2005_jan.createOrReplaceTempView(\"df_view_2005_jan\")\n",
    "\n",
    "# Subreddit Growth: How many unique subreddits were there at the beginning of 2018?\n",
    "active_user_2005_jan = spark.sql(\"SELECT DISTINCT author AS active_user FROM df_view_2005_jan\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18613e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# December 2020\n",
    "# So my assumption for active user is a user that post something (comment/subreddit) within the month that we are analyzing.\n",
    "df_2020_dec.createOrReplaceTempView(\"df_view_2020_dec\")\n",
    "\n",
    "# Subreddit Growth: How many unique subreddits were there at the beginning of 2018?\n",
    "active_user_2020_dec = spark.sql(\"SELECT DISTINCT author AS active_user FROM df_view_2020_dec\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1971b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of active users at jan 2005: 394\n",
      "The number of active users at dec 2020: 2906214\n",
      "Growth percentage: 99.98644284281887\n"
     ]
    }
   ],
   "source": [
    "# Print output answer\n",
    "print(\"The number of active users at jan 2005: {}\".format(active_user_2005_jan))\n",
    "print(\"The number of active users at dec 2020: {}\".format(active_user_2020_dec))\n",
    "print(\"Growth percentage: {}\".format((active_user_2020_dec - active_user_2005_jan)/active_user_2020_dec * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758b6f45",
   "metadata": {},
   "source": [
    "# Best Comment Award: Choose a particular day and determine what the most upvoted comment was. (Include the comment in your report, of course!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "280c9e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max_score|\n",
      "+---------+\n",
      "|    26280|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding the the maximum score on 12-1-2020\n",
    "spark.sql(\"SELECT MAX(score) as max_score FROM (SELECT * FROM df_view_2020_dec WHERE from_unixtime(created_utc, 'MM dd yyyy') == '12 01 2020') AS t\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e41faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|  author|most_upvoted_comment|\n",
      "+--------+--------------------+\n",
      "|ThatKiwi|I've been working...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the most upvoted comment and author\n",
    "most_upvoted_comment = spark.sql(\"SELECT author, body as most_upvoted_comment FROM df_view_2020_dec WHERE score == 26280 AND from_unixtime(created_utc, 'MM dd yyyy') == '12 01 2020'\")\n",
    "most_upvoted_comment.cache()\n",
    "most_upvoted_comment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc3cbbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author:  ThatKiwi\n",
      "comment:  I've been working from home and as my employer, like many others, are soul sucking demons not fit for this earth, they track you in just about every way possible because it's simply inconceivable to think people would just do the work they're paid to do, regardless of the setting they're in.\n",
      "\n",
      "If I dont interact with the computer for more than five minutes it registers me as away, heavens forbid I have to poop or something. \n",
      "\n",
      "I got spoken to the very first week of WFH and told \"you have to be available during work hours\" by a supervisor who is seemingly never available but was blind to the irony I guess.\n",
      "\n",
      "In any event, I found that if you navigate to a certain portion of our 1970s software you can endlessly type. I promptly went outside and found a rock which now holds down my space bar for me anytime I need to walk away for a minute. \n",
      "\n",
      "That rock is, in my eyes, now one of the most important objects in my world and it was free.\n",
      "\n",
      "Edit: Fixed typos. I feel shame. Thank you for the awards! To the people who messaged me offering alternatives, I appreciate your kindness. The reality is I kinda like the rock. I dont see the need for a piece of hardware or a program when the rock works fine. There is something to be said for simplicity and I dont think its gonna get any simpler than a rock. It's basically an on/off switch.\n",
      "\n",
      "No I'm not a horrible employee scamming the system. No matter the job, no one works nonstop a full shift. I do not believe people are wired to complete boring and repetitive tasks endlessly like robots. Everyone needs time to breathe and relax. If I'm stepping away for 6 minutes to make a pot of coffee and poop, I'm a normal human doing normal human things, which I would also do in the office. I'm not setting the rock down and going on a 45 minute adventure to the beach or something.\n",
      "\n",
      "Yes, I work for a horrible employer. I'm looking for something different. I'm sure there are not horrible employers out there. Maybe you are one of them. However, as more places transition to WFH, there seems to be this misconception that people need to adhere to crazier and crazier methods of proving productivity. If you have a good amount of employees you have bad employees. Ones who will abuse the system and not get their work done. This is the case regardless of the setting. In trying to stop these dead weight employees, you're putting everyone else through hell. Just keep that in mind.\n"
     ]
    }
   ],
   "source": [
    "# Printing the whole comment\n",
    "author_comment = most_upvoted_comment.collect()\n",
    "print(\"author: \", author_comment[0][0])\n",
    "print(\"comment: \", author_comment[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc81a57",
   "metadata": {},
   "source": [
    "For this question, I use a reservoir sample for December 2022 data set. The particular date that I chose is December 1st, 2020. To answer this question, I filtered out all the comments that are not created on December 1st, 2020. Then, to get the most upvote, I simply use max funtion to the score column. Lastly, get the comments on that day which has n number of upvotes. The result should show all the comments that receive that number of upvotes.\n",
    "\n",
    "The most popular comment on December 1st, 2020 is a comment/post posted by ThatKiwi. I use reservoir sample for this question too, which makes this comment might not be the most popular on that day. But this comment should be one of the most popular one on that day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582d9a3",
   "metadata": {},
   "source": [
    "# Top Comments: For the user you found in the previous question, find their five most-upvoted comments overall across the entire dataset. Do they post highly-upvoted comments often, or are they a “one hit wonder?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220a3adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts for month 12: 1\n",
      "Number of posts for month 11: 0\n",
      "Number of posts for month 10: 0\n",
      "Number of posts for month 9: 1\n",
      "Number of posts for month 8: 1\n",
      "Number of posts for month 7: 0\n",
      "post: I've been working from home and as my employer, like many others, are soul sucking demons not fit for this earth, they track you in just about every way possible because it's simply inconceivable to think people would just do the work they're paid to do, regardless of the setting they're in.\n",
      "\n",
      "If I dont interact with the computer for more than five minutes it registers me as away, heavens forbid I have to poop or something. \n",
      "\n",
      "I got spoken to the very first week of WFH and told \"you have to be available during work hours\" by a supervisor who is seemingly never available but was blind to the irony I guess.\n",
      "\n",
      "In any event, I found that if you navigate to a certain portion of our 1970s software you can endlessly type. I promptly went outside and found a rock which now holds down my space bar for me anytime I need to walk away for a minute. \n",
      "\n",
      "That rock is, in my eyes, now one of the most important objects in my world and it was free.\n",
      "\n",
      "Edit: Fixed typos. I feel shame. Thank you for the awards! To the people who messaged me offering alternatives, I appreciate your kindness. The reality is I kinda like the rock. I dont see the need for a piece of hardware or a program when the rock works fine. There is something to be said for simplicity and I dont think its gonna get any simpler than a rock. It's basically an on/off switch.\n",
      "\n",
      "No I'm not a horrible employee scamming the system. No matter the job, no one works nonstop a full shift. I do not believe people are wired to complete boring and repetitive tasks endlessly like robots. Everyone needs time to breathe and relax. If I'm stepping away for 6 minutes to make a pot of coffee and poop, I'm a normal human doing normal human things, which I would also do in the office. I'm not setting the rock down and going on a 45 minute adventure to the beach or something.\n",
      "\n",
      "Yes, I work for a horrible employer. I'm looking for something different. I'm sure there are not horrible employers out there. Maybe you are one of them. However, as more places transition to WFH, there seems to be this misconception that people need to adhere to crazier and crazier methods of proving productivity. If you have a good amount of employees you have bad employees. Ones who will abuse the system and not get their work done. This is the case regardless of the setting. In trying to stop these dead weight employees, you're putting everyone else through hell. Just keep that in mind. | upvotes: 26280\n",
      "post: I dunno what that was. But it made me sad. \n",
      "\n",
      "Everyone played poorly. | upvotes: 3\n",
      "post: I recently took my course in NY (although I'm not sure how different it is in Canada) and had absolutely no experience prior to the class with the exception of a very small amount of riding a 50cc automatic moped and being able to drive a standard car. \n",
      "\n",
      "My class had 5 people in it and I was the absolute only one with no experience. I actually felt pressured to lie when I arrived the first day and everyone was talking with the instructor about previous riding they did, but I decided that I had no experience and I was just going to be upfront and honest about that. \n",
      "\n",
      "I didnt have some of the initial struggles you did and I will say my instructor was horrible. I kept asking him how I could do things better and he would give half hearted answers. It was not confidence inspiring. On top of this, I had to deal with the pressure of everyone else seemingly not struggling with anything at all. What I came to find out was that none of my classmates cared that I had no experience. It's a BEGINNER course. That first day my classmates we're just excited to be riding bikes around all day and having fun, just like me. One of them even yelled out a pointer to me when I was struggling. I left the first day disappointed and thinking there was a decent chance I'd fail the test. Everyone in the class was talking about how they couldn't wait to get home and practice some things on their bikes, but I didnt have a bike so I couldn't practice the things I struggled with.\n",
      "\n",
      "You gotta make peace with that feeling. It is what it is. If you were taking you drivers test in a car after driving a car for 8 total hours are you 100% sure you would pass? Loads of people would fail with such little experience! All you can do is try your best and if you fail then you'll retest. Yes it will suck to spend more money, but isnt the main point here that you want to be safe on the road? If it costs an extra $100 or whatever to be safer, that doesnt seem like such a bad deal.\n",
      "\n",
      "The second day was completely different. We had a different instructor who was fantastic. He would actively give me pointers as I was riding and after most of the exercises. He was laid back and it made all of us laid back. In terms of the actual test, I swear it was dramatically easier then the exercises they put you through during the course. It all seemed like a breeze. \n",
      "\n",
      "No one in my class failed and the instructor told everyone after we passed that the goal shouldnt be to get everyone ripping through the streets, but to get you legal and capable of practicing your weak points on your own in a parking lot. He said the most important aspect of riding a bike is to know your weak points and to be able to self analyze and practice the things your worse on. \n",
      "\n",
      "As someone with a lot of anxiety, who was 90% sure he'd fail, dont compare yourself to everyone else in the class. They're just trying to have fun too. Keep your priorities in check and have fun. Sorry about your car door though! | upvotes: 1\n"
     ]
    }
   ],
   "source": [
    "is_done = False # A boolean to exit the loop\n",
    "counter = 0\n",
    "most_popular_posts = {} # Map to store the comment and number of upvotes\n",
    "\n",
    "num_of_years = 0 # A counter of how many years we have done so far\n",
    "num_of_months = 0 # A counter of how many months we have done so far\n",
    "\n",
    "# Since we have a really big data, it takes a long time to process all the years from 2004-2020, therefore\n",
    "# these two variables below is to limit how many years and months do you want to scan\n",
    "num_of_years_target = 1 \n",
    "num_of_months_target = 6\n",
    "\n",
    "# Iterate through the number of years we have which is 2004 - 2020\n",
    "for i in range(20, 4, -1):\n",
    "    if is_done or num_of_years == num_of_years_target:\n",
    "        break\n",
    "    \n",
    "    # Iterate through the number of months 1 - 12\n",
    "    for j in range(12, 0, -1):\n",
    "        if is_done or num_of_months == num_of_months_target:\n",
    "            break\n",
    "            \n",
    "        file_dir = ''\n",
    "        if i == 5:\n",
    "            file_dir = 'hdfs://orion11:23001/RC_2005-12.bz2'\n",
    "        elif i > 5 and i < 18:\n",
    "            file_dir = 'hdfs://orion11:23001/RC_20{:02d}-{:02d}.bz2'.format(i, j)\n",
    "        elif i >= 18:\n",
    "            file_dir = 'hdfs://orion11:23001/RES-RC_20{:02d}-{:02d}.zst'.format(i, j)\n",
    "    \n",
    "        # Read a file from hdfs\n",
    "        df = spark.read.json(file_dir)\n",
    "        df.createOrReplaceTempView(\"df_view\")\n",
    "        \n",
    "        # Get the number of comments/posts 'ThatKiwi' user has\n",
    "        total_post = spark.sql(\"SELECT count(*) AS ct FROM df_view WHERE author == 'ThatKiwi'\")\n",
    "    \n",
    "        print(\"Number of posts for month {}: {}\".format(str(j), str(total_post.take(1)[0]['ct'])))\n",
    "        if (total_post.take(1)[0]['ct'] < 10):\n",
    "            counter += 1\n",
    "            if counter == 7:\n",
    "                is_done: True\n",
    "\n",
    "        # Get the most upvoted comment and store it to the map\n",
    "        most_popular_post = spark.sql(\"SELECT body, score FROM df_view WHERE author == 'ThatKiwi'  ORDER BY score DESC LIMIT 1\")\n",
    "        if (most_popular_post.count() > 0):\n",
    "            post = most_popular_post.take(1)\n",
    "            most_popular_posts[post[0]['body']] = post[0]['score']\n",
    "        \n",
    "        num_of_months += 1\n",
    "    \n",
    "    num_of_years += 1\n",
    "if len(most_popular_posts) == 0:\n",
    "    print(\"The user do not have any other popular posts\")\n",
    "else:\n",
    "    # Print list of comments and upvotes\n",
    "    for key, value in most_popular_posts.items():\n",
    "        print(\"post: \" + key + \" | upvotes: \" + str(value))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f7887",
   "metadata": {},
   "source": [
    "Apparently the user did not post any other comment on December 2020. Additionally, as you can see above from the previous months data, it seems like she/he only occasionally. We can see that within the last 6 months, he/she only posted 3 times. Out of those three comments, only one comments get a lot of upvotes. The rest of the comments only get 1 and 3 upvotes as you can see above. Therefore, it is safe to assume that her/his comment is a \"one hit wonder\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef99d2",
   "metadata": {},
   "source": [
    "# Ban Hammer: Based on user activity, determine which subreddits have been recently banned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07dc68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 2020 november data\n",
    "df_2020_nov = spark.read.json('hdfs://orion11:23001/RES-RC_2020-11.zst')\n",
    "df_2020_nov.createOrReplaceTempView(\"df_view_2020_nov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d695042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           subreddit|\n",
      "+--------------------+\n",
      "|         csci040temp|\n",
      "|          DoomerGvng|\n",
      "|            Vindicta|\n",
      "|dadswhodidnotwant...|\n",
      "|ChiefsvsRavensLiveTv|\n",
      "| RadicalChristianity|\n",
      "|     YoungPrettyHoes|\n",
      "|        megnuttleaks|\n",
      "|        donkybooties|\n",
      "|              hftyty|\n",
      "|               ismos|\n",
      "|FitnessGuidesSharing|\n",
      "|        CallMeCarson|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all subreddits that exist in november 2020 but not exist in december 2020\n",
    "# This approach does not give guarantee that the subreddit is banned because we are using reservoir sample\n",
    "recent_banned_subreddit = spark.sql(\"SELECT DISTINCT subreddit FROM (SELECT subreddit FROM (SELECT subreddit, COUNT(*) AS num_of_comments FROM df_view_2020_nov GROUP BY subreddit) AS t1 WHERE num_of_comments > 100 AND subreddit NOT IN (SELECT DISTINCT subreddit FROM df_view_2020_dec)) AS t3\")\n",
    "\n",
    "recent_banned_subreddit.cache()\n",
    "recent_banned_subreddit.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70434a8b",
   "metadata": {},
   "source": [
    "For this last question, my approach is to compare the subreddit from the month before the current month. The data that I am using for this project is RES-RC_2020-12.zst and RES-RC_2020-11.zst. First, I filtered the subreddit that has at least 100 comments on it. Next, I am comparing the subreddits in November and in December 2020. If the subreddits in November have at least 100 comments and 0 comments in December, it means the subreddit is banned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93271dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
